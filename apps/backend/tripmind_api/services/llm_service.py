# backend/tripmind_api/services/llm_service.py
from __future__ import annotations
import json
import os
import google.generativeai as genai
from flask import current_app

class LLMServiceError(Exception):
    """LLM ì„œë¹„ìŠ¤ ê´€ë ¨ ì—ëŸ¬"""
    pass

class LLMService:
    """Google Gemini LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ íŒŒì‹±í•˜ê±°ë‚˜,
    ëŒ€í™”ì˜ ë¬¸ë§¥ì„ ì´í•´í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."""

    def __init__(self):
        # ì´ˆê¸°í™” ì‹œì ì—ëŠ” ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•Šê³ (Lazy Loading), 
        # ì‹¤ì œ í˜¸ì¶œ ì‹œì ì— current_app contextë¥¼ í†µí•´ í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        self.model = None

    def _get_model(self):
        """ì•± ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if self.model:
            return self.model

        # config.pyì— ì„¤ì •ëœ GEMINI_API_KEY ì‚¬ìš©
        api_key = current_app.config.get("GEMINI_API_KEY")
        
        if not api_key:
             # ê°œë°œ í™˜ê²½ í¸ì˜ë¥¼ ìœ„í•´ os.environë„ í™•ì¸
             api_key = os.environ.get("GEMINI_API_KEY")

        if not api_key:
            raise LLMServiceError("GEMINI_API_KEY not found in app config or environment variables.")
            
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        return self.model

    def _get_system_prompt(self, spec_file_name: str) -> str:
        """ì§€ì •ëœ spec íŒŒì¼ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # '..'ì„ ì‚¬ìš©í•˜ì—¬ 'services' í´ë” ë°–ìœ¼ë¡œ ë‚˜ê°„ í›„ spec íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            spec_path = os.path.join(current_dir, '..', spec_file_name)
            if not os.path.exists(spec_path):
                # íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜í•˜ê±°ë‚˜ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ê°€ëŠ¥
                # ì—¬ê¸°ì„œëŠ” ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¤ë˜, íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¡œì§ì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜
                return "" 
            with open(spec_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê³  ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€)
            print(f"Warning: Failed to load system prompt {spec_file_name}: {e}")
            return ""

    def _call_model(self, prompt: str) -> str:
        """Gemini APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë‚´ë¶€ ë©”ì†Œë“œ"""
        try:
            model = self._get_model()
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            raise LLMServiceError(f"Gemini API Call Failed: {e}")

    # --- ğŸ’¡ 1. 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹ì„ ìœ„í•œ ì‹ ê·œ í•¨ìˆ˜ (í¥ë¯¸ ì¶”ì¶œ) ---
    def extract_interests(self, text):
        """
        ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ì—¬í–‰ ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        """
        prompt = f"""
        Extract travel interest keywords from the text: "{text}"
        Return ONLY a JSON list of strings. Example: ["food", "history"]
        Do not include markdown formatting.
        """
        try:
            result = self._call_model(prompt)
            # JSON íŒŒì‹± ì‹œë„
            cleaned_result = result.replace("```json", "").replace("```", "").strip()
            interests = json.loads(cleaned_result)
            
            # ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°(ì˜ˆ: {"keywords": [...]})ê°€ ì˜¬ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ëª…í™•íˆ ë³€í™˜
            if isinstance(interests, list):
                return interests
            elif isinstance(interests, dict):
                # "keywords" ë˜ëŠ” "interests" í‚¤ê°€ ìˆìœ¼ë©´ ê·¸ ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                if "keywords" in interests and isinstance(interests["keywords"], list):
                    return interests["keywords"]
                if "interests" in interests and isinstance(interests["interests"], list):
                    return interests["interests"]
                
                # íŠ¹ì • í‚¤ê°€ ì—†ìœ¼ë©´ ê°’ë“¤ì„ í‰íƒ„í™”(Flatten)í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¦
                flat_list = []
                for val in interests.values():
                    if isinstance(val, list):
                        flat_list.extend(val)
                    elif isinstance(val, str):
                        flat_list.append(val)
                return flat_list if flat_list else ["general"]
                
            return ["general"]
        except:
            return ["general"]

    # --- ğŸ’¡ 2. 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹ì„ ìœ„í•œ ì‹ ê·œ í•¨ìˆ˜ (êµ­ë‚´/í•´ì™¸ ì¶”ë¡ ) ---
    def check_domestic(self, origin: str, destination: str) -> bool:
        """
        ì¶œë°œì§€ì™€ ë„ì°©ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ­ë‚´/í•´ì™¸ ì—¬ë¶€ë¥¼ JSONìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
        """
        system_prompt = self._get_system_prompt('llm_domestic_spec.md')
        
        # GeminiëŠ” messages ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ì„ í˜¸í•˜ë¯€ë¡œ í•©ì¹©ë‹ˆë‹¤.
        prompt = f"""
        {system_prompt}
        
        Analyze the following trip:
        Origin: {origin}
        Destination: {destination}
        
        Is this a domestic trip within the same country?
        Return JSON only: {{"is_domestic": true/false}}
        """
        
        try:
            result = self._call_model(prompt)
            cleaned_result = result.replace("```json", "").replace("```", "").strip()
            result_json = json.loads(cleaned_result)
            return result_json.get("is_domestic", False) 
        except (json.JSONDecodeError, KeyError, IndexError, TypeError, LLMServiceError) as e:
            print(f"LLMService Error (check_domestic): {e}. Falling back to default (False).")
            # ì¶”ë¡  ì‹¤íŒ¨ ì‹œ 'í•´ì™¸'ë¡œ ê°„ì£¼ (ì•ˆì „í•œ ê¸°ë³¸ê°’)
            return False 

    # --- ğŸ’¡ 3. (ì‹ ê·œ) ì¼ë°˜ ì±„íŒ… í•¨ìˆ˜ (llm.py ë¼ìš°í„°ìš©) ---
    def chat(self, messages: list[dict]) -> str:
        """
        /llm/complete ì—”ë“œí¬ì¸íŠ¸ë¥¼ ìœ„í•œ ë²”ìš© chat í•¨ìˆ˜ì…ë‹ˆë‹¤.
        """
        # messages ë¦¬ìŠ¤íŠ¸ë¥¼ Gemini í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        prompt_parts = []
        for m in messages:
            role = m.get("role", "user")
            content = m.get("content", "")
            prompt_parts.append(f"{role}: {content}")
            
        full_prompt = "\n".join(prompt_parts)
        return self._call_model(full_prompt)
    
    # --- ğŸ’¡ [NEW] ì—¬í–‰ ê³„íš ìˆ˜ì • ê¸°ëŠ¥ ì¶”ê°€ (Gemini ì‚¬ìš©) ---
    def modify_plan(self, current_plan: dict, target_slot: dict, user_prompt: str) -> dict:
        """
        ê¸°ì¡´ ê³„íšê³¼ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • ì¼ì •ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.
        """
        day_idx = target_slot.get('dayIndex')
        event_idx = target_slot.get('eventIndex')
        
        # 1. ìˆ˜ì • ëŒ€ìƒ ì¼ì • ê°€ì ¸ì˜¤ê¸°
        try:
            target_event = current_plan['schedule'][day_idx]['events'][event_idx]
        except (IndexError, KeyError, TypeError):
            raise LLMServiceError("Invalid target slot index or plan structure")

        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        You are a professional travel planner. 
        Your task is to modify a specific travel event based on the user's feedback.
        Return ONLY a valid JSON object representing the modified event.
        The JSON structure must match the 'Current Event' format.

        [Current Event]
        {json.dumps(target_event, ensure_ascii=False)}

        [User Request]
        "{user_prompt}"

        Please provide the modified event as a JSON object.
        Keys required: "time_slot", "description", "icon".
        - "icon" should be one of: "plane", "shopping", "utensils", "home", "coffee", "car".
        Do not include markdown formatting.
        """

        try:
            # 3. LLM í˜¸ì¶œ
            result = self._call_model(prompt)
            cleaned_result = result.replace("```json", "").replace("```", "").strip()
            
            # 4. JSON íŒŒì‹±
            modified_event = json.loads(cleaned_result)
            
            # í•„ìˆ˜ í•„ë“œ ë³´ì • (LLMì´ ëˆ„ë½í–ˆì„ ê²½ìš° ì›ë³¸ ê°’ ì‚¬ìš©)
            if 'time_slot' not in modified_event:
                modified_event['time_slot'] = target_event.get('time_slot')
            if 'icon' not in modified_event:
                modified_event['icon'] = target_event.get('icon', 'map-pin')
                
            return modified_event

        except (json.JSONDecodeError, KeyError, IndexError, LLMServiceError) as e:
            print(f"LLM Modify Error: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„± (ì—ëŸ¬ë¥¼ ë‚´ì§€ ì•Šê³  í…ìŠ¤íŠ¸ë§Œ ë³€ê²½)
            fallback_event = target_event.copy()
            fallback_event['description'] = f"[ìˆ˜ì •ë¨] {user_prompt} (AI ì‘ë‹µ ì‹¤íŒ¨ë¡œ ë‹¨ìˆœ ë°˜ì˜)"
            return fallback_event