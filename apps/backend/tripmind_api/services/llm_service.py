# backend/tripmind_api/services/llm_service.py
from __future__ import annotations
import json
import os
import requests
from ..config import settings

class LLMServiceError(Exception):
    """LLM ì„œë¹„ìŠ¤ ê´€ë ¨ ì—ëŸ¬"""
    pass

class LLMService:
    """Hugging Face LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ íŒŒì‹±í•˜ê±°ë‚˜,
    ëŒ€í™”ì˜ ë¬¸ë§¥ì„ ì´í•´í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."""

    def __init__(self):
        self.session = requests.Session()
        self.hf_token = settings.HF_TOKEN
        self.api_url = f"{settings.HF_BASE_URL}/chat/completions"
        self.model = settings.HF_MODEL

    def _get_system_prompt(self, spec_file_name: str) -> str:
        """ì§€ì •ëœ spec íŒŒì¼ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # ğŸ’¡ '..'ì„ ì‚¬ìš©í•˜ì—¬ 'services' í´ë” ë°–ìœ¼ë¡œ ë‚˜ê°„ í›„ spec íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            spec_path = os.path.join(current_dir, '..', spec_file_name)
            with open(spec_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            raise LLMServiceError(f"LLM spec file '{spec_file_name}' not found at {spec_path}")

    def parse_conversation(self, messages: list[dict]) -> dict:
        """
        [ì‚¬ìš© ì•ˆ í•¨ - 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ë¨]
        ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì •ë³´ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        # (ì´ í•¨ìˆ˜ëŠ” 'trip_route.py'ì˜ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ì—ì„œëŠ” ë” ì´ìƒ í˜¸ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)
        system_prompt = self._get_system_prompt('llm_parser_spec_v2.md')
        
        full_conversation = [{"role": "system", "content": system_prompt}] + messages
        
        llm_response = self._call_llm(full_conversation, response_format={"type": "json_object"})
        
        try:
            content = llm_response['choices'][0]['message']['content']
            return json.loads(content)
        except (json.JSONDecodeError, KeyError, IndexError) as e:
            raise LLMServiceError(f"Failed to parse LLM's JSON response: {e}")

    def generate_clarifying_question(self, messages: list[dict], missing_fields: list[str]) -> str:
        """
        [ì‚¬ìš© ì•ˆ í•¨ - 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ë¨]
        ëˆ„ë½ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë˜ë¬¼ì„ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # (ì´ í•¨ìˆ˜ëŠ” 'trip_route.py'ì˜ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ì—ì„œëŠ” ë” ì´ìƒ í˜¸ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)
        fields_str = ", ".join(missing_fields)
        question_prompt = f"ì—¬í–‰ ê³„íšì— í•„ìš”í•œ ë‹¤ìŒ ì •ë³´({fields_str})ë¥¼ ì–»ê¸° ìœ„í•´, ì¹œì ˆí•œ ì—¬í–‰ ë„ìš°ë¯¸ê°€ ë˜ì–´ ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ í•´ì£¼ì„¸ìš”. ì¸ì‚¬ë‚˜ ë¶€ì—° ì„¤ëª…ì€ ìƒëµí•©ë‹ˆë‹¤."
        
        full_conversation = messages + [{"role": "user", "content": question_prompt}]
        
        response_json = self._call_llm(full_conversation)
        return response_json['choices'][0]['message']['content']

    # --- ğŸ’¡ 1. 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹ì„ ìœ„í•œ ì‹ ê·œ í•¨ìˆ˜ (í¥ë¯¸ ì¶”ì¶œ) ---
    def extract_interests(self, style_text: str) -> list[str]:
        """
        ì‚¬ìš©ìê°€ ì…ë ¥í•œ 'ì—¬í–‰ ìŠ¤íƒ€ì¼ í…ìŠ¤íŠ¸'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í¥ë¯¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
        """
        system_prompt = self._get_system_prompt('llm_interests_spec.md')
        
        # 'parse_conversation'ê³¼ ë‹¬ë¦¬, ì „ì²´ ëŒ€í™”ê°€ ì•„ë‹Œ 'style_text'ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": style_text}
        ]
        
        # JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ìš”ì²­
        llm_response = self._call_llm(messages, response_format={"type": "json_object"})
        
        try:
            content = llm_response['choices'][0]['message']['content']
            # LLMì´ JSON ë¬¸ìì—´(ì˜ˆ: '["íœ´ì–‘", "ë§›ì§‘"]')ì„ ë°˜í™˜í•˜ë©´, ì´ë¥¼ íŒŒì‹±í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            return json.loads(content) 
        except (json.JSONDecodeError, KeyError, IndexError, TypeError) as e:
            print(f"LLMService Error (extract_interests): {e}. Falling back to default.")
            return ["ê´€ê´‘"] # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜

    # --- ğŸ’¡ 2. 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹ì„ ìœ„í•œ ì‹ ê·œ í•¨ìˆ˜ (êµ­ë‚´/í•´ì™¸ ì¶”ë¡ ) ---
    def check_domestic(self, origin: str, destination: str) -> bool:
        """
        ì¶œë°œì§€ì™€ ë„ì°©ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ­ë‚´/í•´ì™¸ ì—¬ë¶€ë¥¼ JSONìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
        """
        system_prompt = self._get_system_prompt('llm_domestic_spec.md')
        
        user_prompt = f"({origin}, {destination})"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ìš”ì²­
        llm_response = self._call_llm(messages, response_format={"type": "json_object"})
        
        try:
            content = llm_response['choices'][0]['message']['content']
            # LLMì´ JSON ë¬¸ìì—´(ì˜ˆ: '{"is_domestic": false}')ì„ ë°˜í™˜í•˜ë©´, íŒŒì‹±í•¨
            result_json = json.loads(content)
            return result_json.get("is_domestic", False) # is_domestic ê°’ì„ boolë¡œ ë°˜í™˜
        except (json.JSONDecodeError, KeyError, IndexError, TypeError) as e:
            print(f"LLMService Error (check_domestic): {e}. Falling back to default (False).")
            # ğŸ’¡ ì¶”ë¡  ì‹¤íŒ¨ ì‹œ 'í•´ì™¸'ë¡œ ê°„ì£¼ (ì•ˆì „í•œ ê¸°ë³¸ê°’)
            return False 

    # --- ğŸ’¡ 3. (ì‹ ê·œ) ì¼ë°˜ ì±„íŒ… í•¨ìˆ˜ (llm.py ë¼ìš°í„°ìš©) ---
    def chat(self, messages: list[dict]) -> str:
        """
        /llm/complete ì—”ë“œí¬ì¸íŠ¸ë¥¼ ìœ„í•œ ë²”ìš© chat í•¨ìˆ˜ì…ë‹ˆë‹¤. (ë™ê¸°)
        """
        # ì´ í•¨ìˆ˜ëŠ” JSON ëª¨ë“œê°€ ì•„ë‹Œ ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ê°€ì •í•©ë‹ˆë‹¤.
        response_json = self._call_llm(messages)
        try:
            return response_json['choices'][0]['message']['content']
        except (KeyError, IndexError) as e:
            raise LLMServiceError(f"Failed to parse LLM's chat response: {e}")
    
    # --- ğŸ’¡ [NEW] ì—¬í–‰ ê³„íš ìˆ˜ì • ê¸°ëŠ¥ ì¶”ê°€ (Hugging Face ì‚¬ìš©) ---
    def modify_plan(self, current_plan: dict, target_slot: dict, user_prompt: str) -> dict:
        """
        ê¸°ì¡´ ê³„íšê³¼ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • ì¼ì •ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.
        """
        day_idx = target_slot.get('dayIndex')
        event_idx = target_slot.get('eventIndex')
        
        # 1. ìˆ˜ì • ëŒ€ìƒ ì¼ì • ê°€ì ¸ì˜¤ê¸°
        try:
            target_event = current_plan['schedule'][day_idx]['events'][event_idx]
        except (IndexError, KeyError, TypeError):
            raise LLMServiceError("Invalid target slot index or plan structure")

        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """
        You are a professional travel planner. 
        Your task is to modify a specific travel event based on the user's feedback.
        Return ONLY a valid JSON object representing the modified event.
        The JSON structure must match the 'Current Event' format.
        """

        user_message = f"""
        [Current Event]
        {json.dumps(target_event, ensure_ascii=False)}

        [User Request]
        "{user_prompt}"

        Please provide the modified event as a JSON object.
        Keys required: "time_slot", "description", "icon".
        - "icon" should be one of: "plane", "shopping", "utensils", "home", "coffee", "car".
        """

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        try:
            # 3. LLM í˜¸ì¶œ (JSON ëª¨ë“œ)
            llm_response = self._call_llm(messages, response_format={"type": "json_object"})
            content = llm_response['choices'][0]['message']['content']
            
            # 4. JSON íŒŒì‹±
            modified_event = json.loads(content)
            
            # í•„ìˆ˜ í•„ë“œ ë³´ì • (LLMì´ ëˆ„ë½í–ˆì„ ê²½ìš° ì›ë³¸ ê°’ ì‚¬ìš©)
            if 'time_slot' not in modified_event:
                modified_event['time_slot'] = target_event.get('time_slot')
            if 'icon' not in modified_event:
                modified_event['icon'] = target_event.get('icon', 'map-pin')
                
            return modified_event

        except (json.JSONDecodeError, KeyError, IndexError, LLMServiceError) as e:
            print(f"LLM Modify Error: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„± (ì—ëŸ¬ë¥¼ ë‚´ì§€ ì•Šê³  í…ìŠ¤íŠ¸ë§Œ ë³€ê²½)
            fallback_event = target_event.copy()
            fallback_event['description'] = f"[ìˆ˜ì •ë¨] {user_prompt} (AI ì‘ë‹µ ì‹¤íŒ¨ë¡œ ë‹¨ìˆœ ë°˜ì˜)"
            return fallback_event

    # --- ë‚´ë¶€ LLM í˜¸ì¶œ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ---
    def _call_llm(self, messages: list[dict], response_format: dict | None = None) -> dict:
        """LLM APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë‚´ë¶€ ë©”ì†Œë“œ (ë™ê¸°)"""
        # ğŸ’¡ ê¸°ì¡´ì˜ HF_TOKEN ì¸ì¦ ë°©ì‹ ìœ ì§€
        headers = {
            "Authorization": f"Bearer {self.hf_token}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.7, # ì°½ì˜ì„± ì¡°ì ˆ
            "max_tokens": 500
        }
        if response_format:
            payload["response_format"] = response_format
        
        try:
            response = self.session.post(self.api_url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            error_details = e.response.text if e.response else str(e)
            # 401 Unauthorized ì—ëŸ¬ê°€ ì—¬ê¸°ì„œ ë°œìƒí•˜ë©´ .envì˜ HF_TOKENì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
            raise LLMServiceError(f"Failed to call LLM API: {error_details}")