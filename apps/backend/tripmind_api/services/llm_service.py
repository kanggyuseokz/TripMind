# backend/tripmind_api/services/llm_service.py
from __future__ import annotations
import json
import os
import google.generativeai as genai
from flask import current_app

class LLMServiceError(Exception):
    """LLM ì„œë¹„ìŠ¤ ê´€ë ¨ ì—ëŸ¬"""
    pass

class LLMService:
    """Google Gemini LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ íŒŒì‹±í•˜ê±°ë‚˜,
    ëŒ€í™”ì˜ ë¬¸ë§¥ì„ ì´í•´í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."""

    def __init__(self):
        # ì´ˆê¸°í™” ì‹œì ì—ëŠ” ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•Šê³ (Lazy Loading), 
        # ì‹¤ì œ í˜¸ì¶œ ì‹œì ì— current_app contextë¥¼ í†µí•´ í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        self.model = None

    def _get_model(self):
        """ì•± ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if self.model:
            return self.model

        # config.pyì— ì„¤ì •ëœ GEMINI_API_KEY ì‚¬ìš©
        api_key = current_app.config.get("GEMINI_API_KEY")
        
        if not api_key:
             # ê°œë°œ í™˜ê²½ í¸ì˜ë¥¼ ìœ„í•´ os.environë„ í™•ì¸
             api_key = os.environ.get("GEMINI_API_KEY")

        if not api_key:
            raise LLMServiceError("GEMINI_API_KEY not found in app config or environment variables.")
            
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        return self.model

    def _get_system_prompt(self, spec_file_name: str) -> str:
        """ì§€ì •ëœ spec íŒŒì¼ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # '..'ì„ ì‚¬ìš©í•˜ì—¬ 'services' í´ë” ë°–ìœ¼ë¡œ ë‚˜ê°„ í›„ spec íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            spec_path = os.path.join(current_dir, '..', spec_file_name)
            if not os.path.exists(spec_path):
                # íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜í•˜ê±°ë‚˜ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ê°€ëŠ¥
                # ì—¬ê¸°ì„œëŠ” ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¤ë˜, íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¡œì§ì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜
                return "" 
            with open(spec_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê³  ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€)
            print(f"Warning: Failed to load system prompt {spec_file_name}: {e}")
            return ""

    def _call_model(self, prompt: str) -> str:
        """Gemini APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë‚´ë¶€ ë©”ì†Œë“œ"""
        try:
            model = self._get_model()
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            raise LLMServiceError(f"Gemini API Call Failed: {e}")

    # --- âœ… [NEW] ì‚¬ìš©ì ìš”ì²­ ì „ì²´ íŒŒì‹± ---
    def parse_user_request(self, user_request: str) -> dict:
        """
        ì‚¬ìš©ì ìš”ì²­ì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ì—¬í–‰ ì •ë³´ ì¶”ì¶œ
        - destination, start_date, end_date, party_size, interests ë“±
        
        Args:
            user_request: ì „ì²´ ì—¬í–‰ ì •ë³´ í…ìŠ¤íŠ¸
        
        Returns:
            {
                'origin': 'ì„œìš¸/ì¸ì²œ (ICN)',
                'destination': 'ë„ì¿„/ë‚˜ë¦¬íƒ€',
                'start_date': '2025-12-04',
                'end_date': '2025-12-08',
                'party_size': 2,
                'is_domestic': False,
                'interests': ['ë§›ì§‘']
            }
        """
        try:
            prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­:
{user_request}

JSON í˜•ì‹:
{{
  "origin": "ì¶œë°œì§€ (IATA ì½”ë“œ í¬í•¨)",
  "destination": "ë„ì°©ì§€/ê³µí•­ëª…",
  "start_date": "YYYY-MM-DD",
  "end_date": "YYYY-MM-DD",
  "party_size": ìˆ«ì,
  "is_domestic": true/false,
  "budget_per_person": {{"amount": ìˆ«ì, "currency": "KRW"}},
  "interests": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
  "schedule": [
    {{
      "day": 1,
      "date": "Day 1",
      "full_date": "YYYY-MM-DD",
      "events": [
        {{"time_slot": "09:00", "description": "í™œë™ ë‚´ìš©", "icon": "car"}},
        {{"time_slot": "12:00", "description": "ì ì‹¬ ì‹ì‚¬", "icon": "utensils"}},
        {{"time_slot": "14:00", "description": "ì˜¤í›„ í™œë™", "icon": "camera"}},
        {{"time_slot": "18:00", "description": "ì €ë… ì‹ì‚¬", "icon": "utensils"}},
        {{"time_slot": "21:00", "description": "í˜¸í…” ë³µê·€", "icon": "home"}}
      ]
    }}
  ]
}}

ì¤‘ìš”:
1. schedule ë°°ì—´ì€ start_dateë¶€í„° end_dateê¹Œì§€ ëª¨ë“  ë‚ ì§œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
2. ê° ë‚ ì˜ eventsëŠ” ìµœì†Œ 3~5ê°œ í¬í•¨í•˜ì„¸ìš”.
3. iconì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜: "plane", "car", "shopping", "utensils", "coffee", "home", "camera"
4. ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.

"""
            
            print(f"[LLMService] ğŸ“ Parsing user request...")
            
            result = self._call_model(prompt)
            cleaned_result = result.replace("```json", "").replace("```", "").strip()
            
            parsed = json.loads(cleaned_result)

            if not parsed.get('schedule'):
                print(f"[LLMService] âš ï¸ No schedule from LLM, will be generated by MCP")
            else:
                print(f"[LLMService] âœ… Schedule generated: {len(parsed['schedule'])} days")

            print(f"[LLMService] âœ… Parsed Request: {parsed}")
            
            return parsed
            
        except Exception as e:
            print(f"[LLMService] âŒ parse_user_request error: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                'destination': 'ë„ì¿„',
                'start_date': '2025-12-04',
                'end_date': '2025-12-08',
                'party_size': 2,
                'is_domestic': False,
                'interests': ['ê´€ê´‘']
            }

    # --- ğŸ’¡ 1. 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹ì„ ìœ„í•œ ì‹ ê·œ í•¨ìˆ˜ (í¥ë¯¸ ì¶”ì¶œ) ---
    def extract_interests(self, text: str) -> list:
        """
        âœ… MD íŒŒì¼(llm_interests_spec.md)ì„ ì‚¬ìš©í•˜ì—¬ ì—¬í–‰ ìŠ¤íƒ€ì¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            text: "ë§›ì§‘ ìœ„ì£¼, íœ´ì–‘ ì„ í˜¸, ë¹¡ë¹¡í•œ ì¼ì •..."
        
        Returns:
            ["ë§›ì§‘", "íœ´ì–‘"]
        """
        try:
            # âœ… MD íŒŒì¼ ë¡œë“œ
            system_prompt = self._get_system_prompt('llm_interests_spec.md')
            
            if not system_prompt:
                # í´ë°±: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
                system_prompt = """ë‹¹ì‹ ì€ ì—¬í–‰ ìŠ¤íƒ€ì¼ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì—¬í–‰ ìŠ¤íƒ€ì¼ í…ìŠ¤íŠ¸ê°€ ì£¼ì–´ì§€ë©´, ['ê´€ê´‘', 'ë§›ì§‘', 'ì‡¼í•‘', 'íœ´ì–‘', 'ì•¡í‹°ë¹„í‹°', 'ë¬¸í™”/ì˜ˆìˆ ', 'ì—­ì‚¬', 'ìì—°'] ì¤‘ì—ì„œ
ê°€ì¥ ê´€ë ¨ ìˆëŠ” í‚¤ì›Œë“œë¥¼ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
ë§Œì•½ íŠ¹ë³„í•œ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ['ê´€ê´‘']ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
            
            # âœ… ì „ì²´ í”„ë¡¬í”„íŠ¸ ìƒì„±
            full_prompt = f"""{system_prompt}

ì‚¬ìš©ì ì…ë ¥: "{text}"

JSON ë¦¬ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì˜ˆ: ["ê´€ê´‘", "ë§›ì§‘"]
"""
            
            print(f"[LLMService] ğŸ¨ Extracting interests: {text}")
            
            result = self._call_model(full_prompt)
            cleaned_result = result.replace("```json", "").replace("```", "").strip()
            
            interests = json.loads(cleaned_result)
            
            # âœ… ë‹¤ì–‘í•œ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
            if isinstance(interests, list):
                print(f"[LLMService] âœ… Extracted Interests: {interests}")
                return interests
            elif isinstance(interests, dict):
                # "keywords" ë˜ëŠ” "interests" í‚¤ê°€ ìˆìœ¼ë©´ ê·¸ ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                if "keywords" in interests and isinstance(interests["keywords"], list):
                    return interests["keywords"]
                if "interests" in interests and isinstance(interests["interests"], list):
                    return interests["interests"]
                
                # íŠ¹ì • í‚¤ê°€ ì—†ìœ¼ë©´ ê°’ë“¤ì„ í‰íƒ„í™”(Flatten)í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¦
                flat_list = []
                for val in interests.values():
                    if isinstance(val, list):
                        flat_list.extend(val)
                    elif isinstance(val, str):
                        flat_list.append(val)
                return flat_list if flat_list else ["ê´€ê´‘"]
                
            return ["ê´€ê´‘"]
            
        except Exception as e:
            print(f"[LLMService] âŒ extract_interests error: {e}. Falling back to ['ê´€ê´‘']")
            return ["ê´€ê´‘"]

    # --- ğŸ’¡ 2. 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹ì„ ìœ„í•œ ì‹ ê·œ í•¨ìˆ˜ (êµ­ë‚´/í•´ì™¸ ì¶”ë¡ ) ---
    def check_domestic(self, origin: str, destination: str) -> bool:
        """
        ì¶œë°œì§€ì™€ ë„ì°©ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ­ë‚´/í•´ì™¸ ì—¬ë¶€ë¥¼ JSONìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
        """
        system_prompt = self._get_system_prompt('llm_domestic_spec.md')
        
        # GeminiëŠ” messages ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ì„ í˜¸í•˜ë¯€ë¡œ í•©ì¹©ë‹ˆë‹¤.
        prompt = f"""
        {system_prompt}
        
        Analyze the following trip:
        Origin: {origin}
        Destination: {destination}
        
        Is this a domestic trip within the same country?
        Return JSON only: {{"is_domestic": true/false}}
        """
        
        try:
            result = self._call_model(prompt)
            cleaned_result = result.replace("```json", "").replace("```", "").strip()
            result_json = json.loads(cleaned_result)
            
            is_domestic = result_json.get("is_domestic", False)
            print(f"[LLMService] ğŸŒ check_domestic: {origin} â†’ {destination} = {is_domestic}")
            
            return is_domestic
        except (json.JSONDecodeError, KeyError, IndexError, TypeError, LLMServiceError) as e:
            print(f"LLMService Error (check_domestic): {e}. Falling back to default (False).")
            # ì¶”ë¡  ì‹¤íŒ¨ ì‹œ 'í•´ì™¸'ë¡œ ê°„ì£¼ (ì•ˆì „í•œ ê¸°ë³¸ê°’)
            return False 

    # --- ğŸ’¡ 3. (ì‹ ê·œ) ì¼ë°˜ ì±„íŒ… í•¨ìˆ˜ (llm.py ë¼ìš°í„°ìš©) ---
    def chat(self, messages: list[dict]) -> str:
        """
        /llm/complete ì—”ë“œí¬ì¸íŠ¸ë¥¼ ìœ„í•œ ë²”ìš© chat í•¨ìˆ˜ì…ë‹ˆë‹¤.
        """
        # messages ë¦¬ìŠ¤íŠ¸ë¥¼ Gemini í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        prompt_parts = []
        for m in messages:
            role = m.get("role", "user")
            content = m.get("content", "")
            prompt_parts.append(f"{role}: {content}")
            
        full_prompt = "\n".join(prompt_parts)
        return self._call_model(full_prompt)
    
    # --- ğŸ’¡ [NEW] ì—¬í–‰ ê³„íš ìˆ˜ì • ê¸°ëŠ¥ ì¶”ê°€ (Gemini ì‚¬ìš©) ---
    def modify_plan(self, current_plan: dict, target_slot: dict, user_prompt: str) -> dict:
        """
        ê¸°ì¡´ ê³„íšê³¼ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • ì¼ì •ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.
        """
        day_idx = target_slot.get('dayIndex')
        event_idx = target_slot.get('eventIndex')
        
        # 1. ìˆ˜ì • ëŒ€ìƒ ì¼ì • ê°€ì ¸ì˜¤ê¸°
        try:
            target_event = current_plan['schedule'][day_idx]['events'][event_idx]
        except (IndexError, KeyError, TypeError):
            raise LLMServiceError("Invalid target slot index or plan structure")

        # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        You are a professional travel planner. 
        Your task is to modify a specific travel event based on the user's feedback.
        Return ONLY a valid JSON object representing the modified event.
        The JSON structure must match the 'Current Event' format.

        [Current Event]
        {json.dumps(target_event, ensure_ascii=False)}

        [User Request]
        "{user_prompt}"

        Please provide the modified event as a JSON object.
        Keys required: "time_slot", "description", "icon".
        - "icon" should be one of: "plane", "shopping", "utensils", "home", "coffee", "car".
        Do not include markdown formatting.
        """

        try:
            # 3. LLM í˜¸ì¶œ
            result = self._call_model(prompt)
            cleaned_result = result.replace("```json", "").replace("```", "").strip()
            
            # 4. JSON íŒŒì‹±
            modified_event = json.loads(cleaned_result)
            
            # í•„ìˆ˜ í•„ë“œ ë³´ì • (LLMì´ ëˆ„ë½í–ˆì„ ê²½ìš° ì›ë³¸ ê°’ ì‚¬ìš©)
            if 'time_slot' not in modified_event:
                modified_event['time_slot'] = target_event.get('time_slot')
            if 'icon' not in modified_event:
                modified_event['icon'] = target_event.get('icon', 'map-pin')
                
            return modified_event

        except (json.JSONDecodeError, KeyError, IndexError, LLMServiceError) as e:
            print(f"LLM Modify Error: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„± (ì—ëŸ¬ë¥¼ ë‚´ì§€ ì•Šê³  í…ìŠ¤íŠ¸ë§Œ ë³€ê²½)
            fallback_event = target_event.copy()
            fallback_event['description'] = f"[ìˆ˜ì •ë¨] {user_prompt} (AI ì‘ë‹µ ì‹¤íŒ¨ë¡œ ë‹¨ìˆœ ë°˜ì˜)"
            return fallback_event